{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fake_News_Detector_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNo1PbngosZ7QDzwpf/C2B0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/angelina-tsuboi/Fake_News_Detector_CNN/blob/main/Fake_News_Detector_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cDn50NPlWdD"
      },
      "source": [
        "**Loading IN Some Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWkWzlZRleY6"
      },
      "source": [
        "import os\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import pickle\n",
        "  \n",
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "\n",
        "from sklearn import linear_model\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Download class resources...\n",
        "r = requests.get(\"https://mattfellenz.be/fakenewsdata.zip\")\n",
        "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "z.extractall()\n",
        "\n",
        "basepath = '.'"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YG9QRQo4pnnu"
      },
      "source": [
        "Initializing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsxQD9aWpr91",
        "outputId": "8c7d9f4a-a841-479c-c2c8-e40b5c9aa17a"
      },
      "source": [
        "if not os.path.exists('fakenewsdata/final_data.pkl'):\n",
        "\tprint('no saved data was found; generating from scratch...')\n",
        "\tprint('loading data')\n",
        "\t# structure of each item: url, html, (1 if fake else 0)\n",
        "\twith open('fakenewsdata/train_val_data.pkl', 'rb') as f:\n",
        "\t\ttrain_data, val_data = pickle.load(f)\n",
        "\twith open('fakenewsdata/test_data.pkl', 'rb') as f:\n",
        "\t\ttest_data = pickle.load(f)\n",
        "\n",
        "\tprint('making Tokenizer')\n",
        "\ttokenizer = Tokenizer(\n",
        "\t\tnum_words=12_000,  # TUNABLE\n",
        "\t\tfilters='!\"#$%&()*+,-./…‘’“”—–:;<=>?@[\\\\]^_`{|}~\\t\\n©®™',\n",
        "\t\tlower=True,\n",
        "\t\tsplit=\" \"\n",
        "\t)\n",
        "\n",
        "\ttrain_data.pop(232)  # for some reason they cause the parser to hang\n",
        "\ttrain_data.pop(301)\n",
        "\ttrain_data.pop(620)\n",
        "\ttrain_data.pop(1362)\n",
        "\ttrain_data.pop(1656)\n",
        "\ttrain_data.pop(1738)\n",
        "\n",
        "\tif not os.path.exists('fakenewsdata/text_data.pkl'):\n",
        "\t\tprint('no saved text found; converting HTML to text')\n",
        "\t\ttrain_texts = [bs(page[1], 'html.parser').get_text() for page in train_data]\n",
        "\t\tvalid_texts = [bs(page[1], 'html.parser').get_text() for page in val_data]\n",
        "\t\ttest_texts = [bs(page[1], 'html.parser').get_text() for page in test_data]\n",
        "\n",
        "\t\twith open('fakenewsdata/text_data.pkl', 'wb') as f:\n",
        "\t\t\tpickle.dump((train_texts, valid_texts, test_texts), f)\n",
        "\telse:\n",
        "\t\tprint('using preconverted text')\n",
        "\t\twith open('fakenewsdata/text_data.pkl', 'rb') as f:\n",
        "\t\t\ttrain_texts, valid_texts, test_texts = pickle.load(f)\n",
        "\n",
        "\tprint('fitting Tokenizer')\n",
        "\ttokenizer.fit_on_texts(train_texts)\n",
        "\ttotal_words = len(tokenizer.word_index)\n",
        "\n",
        "\tprint('generating sequences and labels from data/text from earlier')\n",
        "\tX_train = tokenizer.texts_to_sequences(train_texts)\n",
        "\tX_valid = tokenizer.texts_to_sequences(valid_texts)\n",
        "\tX_test = tokenizer.texts_to_sequences(test_texts)\n",
        "\ty_train = [page[2] for page in train_data]\n",
        "\ty_valid = [page[2] for page in val_data]\n",
        "\ty_test = [page[2] for page in test_data]\n",
        "\n",
        "\tprint('pruning bad data')\n",
        "\n",
        "\tto_pop = []\n",
        "\tfor i in range(len(X_train)):\n",
        "\t\tcontent = train_texts[i]\n",
        "\t\tsequence = X_train[i]\n",
        "\t\tif len(sequence) < 15:\n",
        "\t\t\tto_pop.append(i)\n",
        "\t\telif len(sequence) < 30 and ('403' in content or '404' in content or '401' in content or '500' in content or '502' in content or '503' in content):\n",
        "\t\t\tto_pop.append(i)\n",
        "\tfor offset, idx_to_pop in enumerate(to_pop):\n",
        "\t\tX_train.pop(idx_to_pop - offset)  # the array shrinks when we pop, so account for that. This only works since we know the indexes are sorted low-to-high.\n",
        "\t\ty_train.pop(idx_to_pop - offset)\n",
        "\t\t# no need to pop the texts since they're deleted\n",
        "\tdel to_pop, train_texts\n",
        "\tto_pop = []\n",
        "\tfor i in range(len(X_valid)):\n",
        "\t\tcontent = valid_texts[i]\n",
        "\t\tsequence = X_valid[i]\n",
        "\t\tif len(sequence) < 15:\n",
        "\t\t\tto_pop.append(i)\n",
        "\t\telif len(sequence) < 30 and ('403' in content or '404' in content or '401' in content or '500' in content or '502' in content or '503' in content):\n",
        "\t\t\tto_pop.append(i)\n",
        "\tfor offset, idx_to_pop in enumerate(to_pop):\n",
        "\t\tX_valid.pop(idx_to_pop - offset)\n",
        "\t\ty_valid.pop(idx_to_pop - offset)\n",
        "\tdel to_pop, valid_texts\n",
        "\tto_pop = []\n",
        "\tfor i in range(len(X_test)):\n",
        "\t\tcontent = test_texts[i]\n",
        "\t\tsequence = X_test[i]\n",
        "\t\tif len(sequence) < 15:\n",
        "\t\t\tto_pop.append(i)\n",
        "\t\telif len(sequence) < 30 and ('403' in content or '404' in content or '401' in content or '500' in content or '502' in content or '503' in content):\n",
        "\t\t\tto_pop.append(i)\n",
        "\tfor offset, idx_to_pop in enumerate(to_pop):\n",
        "\t\tX_test.pop(idx_to_pop - offset)\n",
        "\t\ty_test.pop(idx_to_pop - offset)\n",
        "\tdel to_pop, test_texts\n",
        "\n",
        "\tword_idx = tokenizer.word_index\n",
        "\tbreakpoint()\n",
        "\tdel val_data, tokenizer, train_data\n",
        "\twith open('fakenewsdata/final_data.pkl', 'wb') as f:\n",
        "\t\tpickle.dump((X_train, y_train, X_valid, y_valid, X_test, y_test, total_words, word_idx), f)\n",
        "else:\n",
        "\tprint('using saved data')\n",
        "\twith open('fakenewsdata/final_data.pkl', 'rb') as f:\n",
        "\t\tX_train, y_train, X_valid, y_valid, X_test, y_test, total_words, word_idx = pickle.load(f)\n",
        "\tdel X_test, y_test\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using saved data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0kJ_wqypRoL"
      },
      "source": [
        "Creating a Logistic Regression Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "NrkjKAtG4_bv",
        "outputId": "a50dbee9-d2df-4347-8842-fd8e73980592"
      },
      "source": [
        "class_rm = linear_model.LogisticRegression()\n",
        "class_rm.fit(np.array(X_train, dtype=np.float), y_train)\n",
        "print(X_train[1])\n",
        "# print(y_train[0])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'list'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-d6925a17cc66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclass_rm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclass_rm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# print(y_train[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaMr8ZeLxU8x"
      },
      "source": [
        "Using Logistic Regression Model to Predict Fake News"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJxRzmymAn9o"
      },
      "source": [
        "y_pred = class_rm.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wotBoqhrA3_0"
      },
      "source": [
        "Visualizing Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJCM6BGQA6E5"
      },
      "source": [
        "y_pred = y_pred.squeeze()\n",
        "x_test_view = x_test[input_labels].values.squeeze()\n",
        "sns.scatterplot(x = x_test_view, y = y_pred, hue = y_test)\n",
        "plt.xlabel('Radius')\n",
        "plt.ylabel('Predicted')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnWFZ4HMBGM7"
      },
      "source": [
        "Helper method to get stats about model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lk3yHJlcBKrS"
      },
      "source": [
        "def model_stats(y_test, y_pred):\n",
        "  print(\"Accuracy: \", metrics.accuracy_score(y_test, y_pred))\n",
        "  print(\"Precision: \", metrics.precision_score(y_test, y_pred))\n",
        "  print(\"Recall: \", metrics.recall_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ns-ZSOLHBLrc"
      },
      "source": [
        "model_stats(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}