{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fake_News_Detector_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPGttSxfnEkwWWz4igYZ24p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/angelina-tsuboi/Fake_News_Detector_CNN/blob/main/Fake_News_Detector_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cDn50NPlWdD"
      },
      "source": [
        "**Loading IN Some Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWkWzlZRleY6"
      },
      "source": [
        "import os\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import pickle\n",
        "  \n",
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import model_selection\n",
        "from sklearn.metrics import accuracy_score\n",
        "from collections import Counter\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D\n",
        "from keras.layers import Activation, MaxPooling2D, Dropout, Flatten, Reshape\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Download class resources...\n",
        "r = requests.get(\"https://www.dropbox.com/s/2pj07qip0ei09xt/inspirit_fake_news_resources.zip?dl=1\")\n",
        "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "z.extractall()\n",
        "\n",
        "basepath = '.'"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YG9QRQo4pnnu"
      },
      "source": [
        "Initializing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "zsxQD9aWpr91",
        "outputId": "941afc4e-c75a-490a-c201-c0e49aaa18dc"
      },
      "source": [
        "if not os.path.exists('final_data.pkl'):\n",
        "\tprint('no saved data was found; generating from scratch...')\n",
        "\tprint('loading data')\n",
        "\t# structure of each item: url, html, (1 if fake else 0)\n",
        "\twith open('fakenewsdata/train_val_data.pkl', 'rb') as f:\n",
        "\t\ttrain_data, val_data = pickle.load(f)\n",
        "\twith open('fakenewsdata/test_data.pkl', 'rb') as f:\n",
        "\t\ttest_data = pickle.load(f)\n",
        "\n",
        "\tprint('making Tokenizer')\n",
        "\ttokenizer = Tokenizer(\n",
        "\t\tnum_words=12_000,  # TUNABLE\n",
        "\t\tfilters='!\"#$%&()*+,-./…‘’“”—–:;<=>?@[\\\\]^_`{|}~\\t\\n©®™',\n",
        "\t\tlower=True,\n",
        "\t\tsplit=\" \"\n",
        "\t)\n",
        "\n",
        "\ttrain_data.pop(232)  # for some reason they cause the parser to hang\n",
        "\ttrain_data.pop(301)\n",
        "\ttrain_data.pop(620)\n",
        "\ttrain_data.pop(1362)\n",
        "\ttrain_data.pop(1656)\n",
        "\ttrain_data.pop(1738)\n",
        "\n",
        "\tif not os.path.exists('text_data.pkl'):\n",
        "\t\tprint('no saved text found; converting HTML to text')\n",
        "\t\ttrain_texts = [bs(page[1], 'html.parser').get_text() for page in train_data]\n",
        "\t\tvalid_texts = [bs(page[1], 'html.parser').get_text() for page in val_data]\n",
        "\t\ttest_texts = [bs(page[1], 'html.parser').get_text() for page in test_data]\n",
        "\n",
        "\t\twith open('fakenewsdata/text_data.pkl', 'wb') as f:\n",
        "\t\t\tpickle.dump((train_texts, valid_texts, test_texts), f)\n",
        "\telse:\n",
        "\t\tprint('using preconverted text')\n",
        "\t\twith open('fakenewsdata/text_data.pkl', 'rb') as f:\n",
        "\t\t\ttrain_texts, valid_texts, test_texts = pickle.load(f)\n",
        "\n",
        "\tprint('fitting Tokenizer')\n",
        "\ttokenizer.fit_on_texts(train_texts)\n",
        "\ttotal_words = len(tokenizer.word_index)\n",
        "\n",
        "\tprint('generating sequences and labels from data/text from earlier')\n",
        "\tX_train = tokenizer.texts_to_sequences(train_texts)\n",
        "\tX_valid = tokenizer.texts_to_sequences(valid_texts)\n",
        "\tX_test = tokenizer.texts_to_sequences(test_texts)\n",
        "\ty_train = [page[2] for page in train_data]\n",
        "\ty_valid = [page[2] for page in val_data]\n",
        "\ty_test = [page[2] for page in test_data]\n",
        "\n",
        "\tprint('pruning bad data')\n",
        "\n",
        "\tto_pop = []\n",
        "\tfor i in range(len(X_train)):\n",
        "\t\tcontent = train_texts[i]\n",
        "\t\tsequence = X_train[i]\n",
        "\t\tif len(sequence) < 15:\n",
        "\t\t\tto_pop.append(i)\n",
        "\t\telif len(sequence) < 30 and ('403' in content or '404' in content or '401' in content or '500' in content or '502' in content or '503' in content):\n",
        "\t\t\tto_pop.append(i)\n",
        "\tfor offset, idx_to_pop in enumerate(to_pop):\n",
        "\t\tX_train.pop(idx_to_pop - offset)  # the array shrinks when we pop, so account for that. This only works since we know the indexes are sorted low-to-high.\n",
        "\t\ty_train.pop(idx_to_pop - offset)\n",
        "\t\t# no need to pop the texts since they're deleted\n",
        "\tdel to_pop, train_texts\n",
        "\tto_pop = []\n",
        "\tfor i in range(len(X_valid)):\n",
        "\t\tcontent = valid_texts[i]\n",
        "\t\tsequence = X_valid[i]\n",
        "\t\tif len(sequence) < 15:\n",
        "\t\t\tto_pop.append(i)\n",
        "\t\telif len(sequence) < 30 and ('403' in content or '404' in content or '401' in content or '500' in content or '502' in content or '503' in content):\n",
        "\t\t\tto_pop.append(i)\n",
        "\tfor offset, idx_to_pop in enumerate(to_pop):\n",
        "\t\tX_valid.pop(idx_to_pop - offset)\n",
        "\t\ty_valid.pop(idx_to_pop - offset)\n",
        "\tdel to_pop, valid_texts\n",
        "\tto_pop = []\n",
        "\tfor i in range(len(X_test)):\n",
        "\t\tcontent = test_texts[i]\n",
        "\t\tsequence = X_test[i]\n",
        "\t\tif len(sequence) < 15:\n",
        "\t\t\tto_pop.append(i)\n",
        "\t\telif len(sequence) < 30 and ('403' in content or '404' in content or '401' in content or '500' in content or '502' in content or '503' in content):\n",
        "\t\t\tto_pop.append(i)\n",
        "\tfor offset, idx_to_pop in enumerate(to_pop):\n",
        "\t\tX_test.pop(idx_to_pop - offset)\n",
        "\t\ty_test.pop(idx_to_pop - offset)\n",
        "\tdel to_pop, test_texts\n",
        "\n",
        "\tword_idx = tokenizer.word_index\n",
        "\tbreakpoint()\n",
        "\tdel val_data, tokenizer, train_data\n",
        "\twith open('fakenewsdata/final_data.pkl', 'wb') as f:\n",
        "\t\tpickle.dump((X_train, y_train, X_valid, y_valid, X_test, y_test, total_words, word_idx), f)\n",
        "else:\n",
        "\tprint('using saved data')\n",
        "\twith open('fakenewsdata/final_data.pkl', 'rb') as f:\n",
        "\t\tX_train, y_train, X_valid, y_valid, X_test, y_test, total_words, word_idx = pickle.load(f)\n",
        "\tdel X_test, y_test\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "no saved data was found; generating from scratch...\n",
            "loading data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-63cba7266c5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loading data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;31m# structure of each item: url, html, (1 if fake else 0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fakenewsdata/train_val_data.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m                 \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fakenewsdata/test_data.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'fakenewsdata/train_val_data.pkl'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0kJ_wqypRoL"
      },
      "source": [
        "Creating a CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFSaOEc7pP9x"
      },
      "source": [
        "def CNNClassifier(num_epochs=2, layers=1, dropout=0.15):\n",
        "  def create_model():\n",
        "    model = Sequential()\n",
        "    model.add(Reshape((32, 32, 3)))\n",
        "    \n",
        "    for i in range(layers):\n",
        "      model.add(Conv2D(32, (3, 3), padding='same'))\n",
        "      model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Conv2D(32, (3, 3)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(dropout))\n",
        "\n",
        "    model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(64, (3, 3)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(dropout))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Dense(2))\n",
        "    model.add(Activation('softmax'))\n",
        "\n",
        "    # initiate RMSprop optimizer\n",
        "    opt = keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)\n",
        "\n",
        "    # Let's train the model using RMSprop\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=opt,\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "  return KerasClassifier(build_fn=create_model, epochs=num_epochs, batch_size=10, verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGUPrFJvx-sj"
      },
      "source": [
        "*Plotting Model Performance*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hO92zjWnyJYw"
      },
      "source": [
        "def plot_acc(history, ax = None, xlabel = 'Epoch #'):\n",
        "    history = history.history\n",
        "    history.update({'epoch':list(range(len(history['val_accuracy'])))})\n",
        "    history = pd.DataFrame.from_dict(history)\n",
        "\n",
        "    best_epoch = history.sort_values(by = 'val_accuracy', ascending = False).iloc[0]['epoch']\n",
        "\n",
        "    if not ax:\n",
        "      f, ax = plt.subplots(1,1)\n",
        "    sns.lineplot(x = 'epoch', y = 'val_accuracy', data = history, label = 'Validation', ax = ax)\n",
        "    sns.lineplot(x = 'epoch', y = 'accuracy', data = history, label = 'Training', ax = ax)\n",
        "    ax.axhline(0.5, linestyle = '--',color='red', label = 'Chance')\n",
        "    ax.axvline(x = best_epoch, linestyle = '--', color = 'green', label = 'Best Epoch')  \n",
        "    ax.legend(loc = 1)    \n",
        "    ax.set_ylim([0.4, 1])\n",
        "\n",
        "    ax.set_xlabel(xlabel)\n",
        "    ax.set_ylabel('Accuracy (Fraction)')\n",
        "    \n",
        "    plt.show()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaMr8ZeLxU8x"
      },
      "source": [
        "Initializing and Using CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "ltCpkRLWxZJ2",
        "outputId": "5a68d075-4694-4beb-db45-778aef5acbcc"
      },
      "source": [
        "cnn = CNNClassifier(5, 2, 0.5)\n",
        "cnn.fit(pad_sequences(tf.convert_to_tensor(X_train, dtype=tf.float32)), y_train)\n",
        "preds = cnn.predict(X_test)\n",
        "print (cnn.score(inputs_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-92d636d355a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCNNClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-a0bee25d0c63>\u001b[0m in \u001b[0;36mCNNClassifier\u001b[0;34m(num_epochs, layers, dropout)\u001b[0m\n\u001b[1;32m     35\u001b[0m                   metrics=['accuracy'])\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mKerasClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuild_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'KerasClassifier' is not defined"
          ]
        }
      ]
    }
  ]
}